<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kamran Janjua</title>
  
  <meta name="author" content="Kamran Janjua">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Muhammad Kamran Janjua</name>
              </p>
              <p style="text-align:center">
                <strong>Art is Long; Time is Fleeting!</strong>
              </p>
              <!-- working in <a href="https://scholar.google.ca/citations?hl=en&user=23J-2m4AAAAJ&view_op=list_works&sortby=pubdate">Dr. Chao Gao's</a> team, previously in <a href="https://scholar.google.ca/citations?hl=en&user=qXuc8RQAAAAJ&view_op=list_works&sortby=pubdate">Dr. Mohammad Salameh's</a> team, at Edmonton Research Center. -->
              <p>I am a Machine Learning Researcher @ Huawei, Canada.
              I work on video understanding and online learning problems.
              Previously, I was a graduate student with <a href="https://scholar.google.ca/citations?view_op=list_works&hl=en&hl=en&user=t5zdD_IAAAAJ">Dr. Martha White</a> at University of Alberta, where I worked on continual learning.
              </p>
              <!-- <p>Previously, I was a graduate student with <a href="https://scholar.google.ca/citations?view_op=list_works&hl=en&hl=en&user=t5zdD_IAAAAJ">Dr. Martha White</a> at University of Alberta, where I worked on continual learning.
                Particularly, I worked on making online, and continual anticipatory predictions in real-time on non-stationary systems. My work is possibly one of the first to show the feasibility of true online TD prediction agent on a real-world industrial plant.
              </p>
              <p>
                Before that, I worked as a Research Associate at Qatar Computing Research Institute (QCRI) working on interpretability in neural language models. Specifically, I worked on understanding how neural language models build (if at all) grammatical structure of language internally.
                I was supervised by <a href="https://scholar.google.co.in/citations?hl=en&user=t3BH6NkAAAAJ">Dr. Hassan Sajjad</a>.
              </p>
              <p>
                Before that I did my undergraduate from National University of Sciences & Technology (NUST), Islamabad working in TUKL-NUST R&D Lab working with <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=o9RCNZYAAAAJ">Dr. Faisal Shafait</a>.
                I have had the pleasure of visiting AaltoVision and AaltoML lab at Aalto University, Finland to work on estimating depth from stereo setups advised by <a href="https://users.aalto.fi/~asolin/">Dr. Arno Solin</a>.
                I have also worked as a research intern at <a href="http://artelab.dista.uninsubria.it/">ARTE Lab, University of Insubria</a> back in 2018 where I worked on designing multi-modal neural networks, and was advised by <a href="https://scholar.google.com/citations?user=zEOwQzIAAAAJ&hl=en">Dr. Ignazio Gallo</a>.
              </p> -->
              <!-- <p><strong>Fun Fact: </strong><a href="https://www.csauthors.net/distance/muhammad-kamran-janjua/paul-erdos">My Erd≈ës Number is 4.</a></p> -->
              <p style="text-align:center">
                <a href="mailto:mjanjua@ualberta.ca">Email</a> &nbsp/&nbsp
                <a href="cv/CV_Kamran_Janjua.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="https://www.quora.com/profile/Kamran-Janjua-1">Quora</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.ca/citations?hl=en&user=UffhXU4AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/kjanjua26/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/kj-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/circle-kj.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in problems in perception and learning, particularly in learning from videos. Some (representative) papers are <mark style="background-color: #ECFAE5;">highlighted</mark>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr bgcolor="#ECFAE5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ronin.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2507.14851">
                <papertitle>Grounding Degradations in Natural Language for All-In-One Video Restoration</papertitle>
              </a>
              <br>
              <strong>Muhammad Kamran Janjua</strong>,
              Amirhosein Ghasemabadi,
              Kunlin Zhang,
              Mohammad Salameh,
              Chao Gao,
              Di Niu
              <br>
              <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2026
              <br>
              <p>We propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/conicgrad.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2502.00217">
                <papertitle>Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone</papertitle>
              </a>
              <br>
              Negar Hassanpour,
              <strong>Muhammad Kamran Janjua</strong>,
              Kunlin Zhang,
              Sepehr Lavasani,
              et. al.
              <br>
              <em>Preprint</em>, 2025
              <br>
              <p>We propose ConicGrad, a principled, scalable, and robust MTL approach formulated as a constrained optimization problem. Our method introduces an angular constraint to dynamically regulate gradient update directions, confining them within a cone centered on the reference gradient of the overall objective.</p>
            </td>
          </tr>


          <tr bgcolor="#ECFAE5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/turtle.gif" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="turtle/index.html">
                <papertitle>Learning Truncated Causal History Model for Video Restoration</papertitle>
              </a>
              <br>
              Amirhosein Ghasemabadi*,
              <strong>Muhammad Kamran Janjua*</strong>,
              Mohammad Salameh, 
              Di Niu (* Equal Contribution)
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br>
              <p>We propose Turtle to learn the truncated causal history model for online video processing. The causal design in Turtle enables recurrence in inference through state-memorized historical features while allowing parallel training by sampling truncated video clips. We report new state-of-the-art results on a multitude of video restoration benchmark tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cgnet.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=C3FXHxMVuq">
                <papertitle>CascadedGaze: Efficiency in Global Context Extraction for Image Restoration</papertitle>
              </a>
              <br>
              Amirhosein Ghasemabadi,
              <strong>Muhammad Kamran Janjua</strong>, 
              Mohammad Salameh, 
              Chunhua Zhou, 
              Fengyu Sun, 
              Di Niu
              <br>
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2024
              <br>
              <p>We present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to learn global information for image restoration.</p>
            </td>
          </tr>

          <tr bgcolor="#ECFAE5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/waterpaper.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s10994-023-06413-x">
                <papertitle>GVFs in the Real World: Making Predictions Online for Water Treatment</papertitle>
              </a>
              <br>
              <strong>Muhammad Kamran Janjua</strong>,
              Haseeb Shah, 
              Martha White, 
              Erfan Miahi, 
              Marlos C Machado, 
              Adam White
              <br>
              <em>Machine Learning</em>, 2023
              <br>
              <p>We show the importance of learning in deployment, by comparing a TD agent trained purely offline with no online updating to a TD agent that learns online. This final result is one of the first to motivate the importance of adapting predictions in real-time, for non-stationary high-volume systems in the real world.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gp-work.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2010.09105.pdf">
                <papertitle>Movement-Induced Priors for Deep Stereo</papertitle>
              </a>
              <br>
              Yuxin Hou,
              <strong>Muhammad Kamran Janjua</strong>,
              Juho Kannala,
              Arno Solin
              <br>
              <em>25th International Conference on Pattern Recognition (ICPR)</em>, 2020
              <br>
              <p>We propose a method for fusing stereo disparity estimation with movement-induced prior information.</p>
            </td>
          </tr>

          <tr bgcolor="#ECFAE5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ssnet.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1909.08685.pdf">
                <papertitle>Deep Latent Space Learning for Cross-Modal Mapping of Audio and Visual Signals</papertitle>
              </a>
              <br>
              <strong>Muhammad Kamran Janjua*</strong>,
              Shah Nawaz*,
              Ignazio Gallo,
              Arif Mahmood,
              Alessandro Calefati
              <br>
              <em>Digital Image Computing: Techniques and Applications (DICTA)</em>, 2019
              <br>
              <p>We propose to learn a joint representation of audio and visual information without relying on multimodal pairs or triplets for matching, verficiation, and retrieval tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/semantic-map.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/CROMOL/Nawaz_Do_Cross_Modal_Systems_Leverage_Semantic_Relationships_ICCVW_2019_paper.pdf">
                <papertitle>Do Cross Modal Systems Leverage Semantic Relationships?</papertitle>
              </a>
              <br>
              Shah Nawaz,
              <strong>Muhammad Kamran Janjua</strong>,
              Ignazio Gallo,
              Arif Mahmood,
              Alessandro Calefati,
              Faisal Shafait
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</em>, 2019
              <br>
              <p>We propose a new measure SemanticMap to evaluate the performance of cross modal systems. Our proposed measure evaluates the semantic similarity between the image and text representations in the latent embedding space.</p>
            </td>
          </tr>

          <tr bgcolor="#ECFAE5">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gitloss.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1807.08512">
                <papertitle>Git Loss for Deep Face Recognition</papertitle>
              </a>
              <br>
              Alessandro Calefati*,
              <strong>Muhammad Kamran Janjua*</strong>,
              Shah Nawaz,
              Ignazio Gallo (* Equal Contribution)
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2018
              <br>
              <!-- <a href="https://github.com/kjanjua26/Git-Loss-For-Deep-Face-Recognition">code</a> -->
              <p>In order to further enhance the discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances.</p>
            </td>
          </tr>

  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Invited Talk(s)</heading>
    </td>
    </tr>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/mmweekly_talk.png" alt="b3do" width="160" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://youtu.be/FSza01RED40?t=1195">
            <papertitle>Learning to Process Streaming Videos Online with Histories</papertitle>
          </a>
          <br>
          <strong>Muhammad Kamran Janjua</strong>
          <br>
          <em>Multimodal Weekly @ TwelveLabs</em>, April 2025
          <br>
        </td>
      </tr>
    </table>
  </table>
</body>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:right;font-size:small;">
        This super cool design (at least I find it to be cool) is taken from <a href="https://jonbarron.info/">&#10025;</a>
      </p>
    </td>
  </tr>
  </tbody>
  </table>

</body>
</html>