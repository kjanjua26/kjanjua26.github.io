<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kamran Janjua</title>
  
  <meta name="author" content="Kamran Janjua">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Muhammad Kamran Janjua</name>
              </p>
              <p style="text-align:center">
                <strong>Art is Long; Time is Fleeting!</strong>
              </p>
              <p>I am a Machine Learning Researcher @ Huawei, Canada working in <a href="https://scholar.google.ca/citations?hl=en&user=23J-2m4AAAAJ&view_op=list_works&sortby=pubdate">Dr. Chao Gao's</a> team, previously in <a href="https://scholar.google.ca/citations?hl=en&user=qXuc8RQAAAAJ&view_op=list_works&sortby=pubdate">Dr. Mohammad Salameh's</a> team, at Edmonton Research Center.</p>
              <p>Previously, I was a graduate student with <a href="https://scholar.google.ca/citations?view_op=list_works&hl=en&hl=en&user=t5zdD_IAAAAJ">Dr. Martha White</a> at University of Alberta, where I worked on representation learning for reinforcement learning.
                Particularly I worked at the combination of offline and online RL where the goal was to learn effective representations in the offline phase to warm-start
                the agent in the online setting and make sure that the agent converges quickly.
              </p>
              <p>
                Before that, I worked as a Research Associate at Qatar Computing Research Institute (QCRI) working on interpretability in neural language models. Specifically, I worked on understanding how neural language models build (if at all) grammatical structure of language internally.
                I was supervised by <a href="https://scholar.google.co.in/citations?hl=en&user=t3BH6NkAAAAJ">Dr. Hassan Sajjad</a>.
              </p>
              <p>
                Before that I did my undergraduate from National University of Sciences & Technology (NUST), Islamabad working in TUKL-NUST R&D Lab working with <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=o9RCNZYAAAAJ">Dr. Faisal Shafait</a>.
                Over the years in this pursuit of science, I have had the pleasure of visiting AaltoVision and AaltoML lab at Aalto University, Finland to work on estimating depth from stereo setups.
                I have also worked as a research intern at <a href="http://artelab.dista.uninsubria.it/">ARTE Lab, University of Insubria</a> back in 2018 where I worked on designing multi-modal neural networks. My research was supervised by <a href="https://scholar.google.com/citations?user=zEOwQzIAAAAJ&hl=en">Dr. Ignazio Gallo</a>.
              </p>
              <p><strong>Fun Fact: </strong><a href="https://www.csauthors.net/distance/muhammad-kamran-janjua/paul-erdos">My Erd≈ës Number is 4.</a></p>
              <p style="text-align:center">
                <a href="mailto:mjanjua@ualberta.com">Email</a> &nbsp/&nbsp
                <a href="cv/CV_Kamran_Janjua.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="https://www.quora.com/profile/Kamran-Janjua-1">Quora</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.ca/citations?user=UffhXU4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/kjanjua26/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/kj-circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/circle-kj.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in online, and continual learning from continuous streams of data (text/tabular/videos).
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cgnet.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=C3FXHxMVuq">
                <papertitle>CascadedGaze: Efficiency in Global Context Extraction for Image Restoration</papertitle>
              </a>
              <br>
              Amirhosein Ghasemabadi,
              <strong>Muhammad Kamran Janjua</strong>, 
              Mohammad Salameh, 
              Chunhua Zhou, 
              Fengyu Sun, 
              Di Niu
              <br>
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2024
              <br>
              <p>We present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to learn global information for image restoration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/waterpaper.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s10994-023-06413-x">
                <papertitle>GVFs in the Real World: Making Predictions Online for Water Treatment</papertitle>
              </a>
              <br>
              <strong>Muhammad Kamran Janjua</strong>,
              Haseeb Shah, 
              Martha White, 
              Erfan Miahi, 
              Marlos C Machado, 
              Adam White
              <br>
              <em>Machine Learning</em>, 2023
              <br>
              <p>We show the importance of learning in deployment, by comparing a TD agent trained purely offline with no online updating to a TD agent that learns online. This final result is one of the first to motivate the importance of adapting predictions in real-time, for non-stationary high-volume systems in the real world.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gp-work.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2010.09105.pdf">
                <papertitle>Movement-Induced Priors for Deep Stereo</papertitle>
              </a>
              <br>
              Yuxin Hou,
              <strong>Muhammad Kamran Janjua</strong>,
              Juho Kannala,
              Arno Solin
              <br>
              <em>25th International Conference on Pattern Recognition (ICPR)</em>, 2020
              <br>
              <p>We propose a method for fusing stereo disparity estimation with movement-induced prior information.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/semantic-map.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/CROMOL/Nawaz_Do_Cross_Modal_Systems_Leverage_Semantic_Relationships_ICCVW_2019_paper.pdf">
                <papertitle>Do Cross Modal Systems Leverage Semantic Relationships?</papertitle>
              </a>
              <br>
              Shah Nawaz,
              <strong>Muhammad Kamran Janjua*</strong>,
              Ignazio Gallo,
              Arif Mahmood,
              Alessandro Calefati,
              Faisal Shafait
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2019
              <br>
              <p>We propose a new measure SemanticMap to evaluate the performance of cross modal systems. Our proposed measure evaluates the semantic similarity between the image and text representations in the latent embedding space.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ssnet.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1909.08685.pdf">
                <papertitle>Deep Latent Space Learning for Cross-Modal Mapping of Audio and Visual Signals</papertitle>
              </a>
              <br>
              Shah Nawaz*,
              <strong>Muhammad Kamran Janjua*</strong>,
              Ignazio Gallo,
              Arif Mahmood,
              Alessandro Calefati
              <br>
              <em>Digital Image Computing: Techniques and Applications (DICTA)</em>, 2019
              <br>
              <p>We propose a novel deep training algorithm for joint representation of audio and visual information which consists of a single stream network (SSNet) coupled with a novel loss function to learn a shared deep latent space representation of multimodal information.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gitloss.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1807.08512">
                <papertitle>Git Loss for Deep Face Recognition</papertitle>
              </a>
              <br>
              <strong>Muhammad Kamran Janjua*</strong>,
              Alessandro Calefati*,
              Shah Nawaz,
              Ignazio Gallo
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2018
              <br>
              <!-- <a href="https://github.com/kjanjua26/Git-Loss-For-Deep-Face-Recognition">code</a> -->
              <p>In order to further enhance the discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances.</p>
            </td>
          </tr>

  </table>
</body>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:right;font-size:small;">
        This super cool design (at least I find it to be cool) is taken from <a href="https://jonbarron.info/">&#10025;</a>
      </p>
    </td>
  </tr>
  </tbody>
  </table>

</body>
</html>